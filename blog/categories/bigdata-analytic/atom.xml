<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Bigdata Analytic | Tony Chou's Blog]]></title>
  <link href="http://yoyzhou.github.com/blog/categories/bigdata-analytic/atom.xml" rel="self"/>
  <link href="http://yoyzhou.github.com/"/>
  <updated>2013-04-20T18:07:14+08:00</updated>
  <id>http://yoyzhou.github.com/</id>
  <author>
    <name><![CDATA[yoyzhou]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[My First Lucky and Sad Hadoop Results]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results/"/>
    <updated>2013-04-20T17:05:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results</id>
    <content type="html"><![CDATA[<p>After a couple of tryings, finally and luckily I gained a completed Hadoop Job results, but with only 1000 lines of records processed.</p>

<p>Here is the Job Summary:</p>

<table>
  <tbody>
    <tr>
      <td>Counter</td>
      <td>Map</td>
      <td>Reduce</td>
      <td>Total</td>
    </tr>
    <tr>
      <td>Bytes Read</td>
      <td>7,945,196</td>
      <td>0</td>
      <td>7,945,196</td>
    </tr>
    <tr>
      <td>FILE_BYTES_READ</td>
      <td>16,590,565,518</td>
      <td>8,021,579,181</td>
      <td>24,612,144,699</td>
    </tr>
    <tr>
      <td><strong>HDFS_BYTES_READ</strong></td>
      <td><strong><em>7,945,580</em></strong></td>
      <td>0</td>
      <td>7,945,580</td>
    </tr>
    <tr>
      <td>FILE_BYTES_WRITTEN</td>
      <td>24,612,303,774</td>
      <td>8,021,632,091</td>
      <td>32,633,935,865</td>
    </tr>
    <tr>
      <td>HDFS_BYTES_WRITTEN</td>
      <td>0</td>
      <td>2,054,409,494</td>
      <td>2,054,409,494</td>
    </tr>
    <tr>
      <td>Reduce input groups</td>
      <td>0</td>
      <td>381,696,888</td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Map output materialized bytes</td>
      <td>8,021,579,181</td>
      <td>0</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Combine output records</td>
      <td>826,399,600</td>
      <td>0</td>
      <td>826,399,600</td>
    </tr>
    <tr>
      <td><strong>Map input records</strong></td>
      <td><strong><em>1,000</em></strong></td>
      <td>0</td>
      <td>1,000</td>
    </tr>
    <tr>
      <td>Reduce shuffle bytes</td>
      <td>0</td>
      <td>8,021,579,181</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Physical memory (bytes) snapshot</td>
      <td>1,215,041,536</td>
      <td>72,613,888</td>
      <td>1,287,655,424</td>
    </tr>
    <tr>
      <td><strong>Reduce output records</strong></td>
      <td>0</td>
      <td><strong><em>381,696,888</em></strong></td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Spilled Records</td>
      <td>1,230,714,511</td>
      <td>401,113,702</td>
      <td>1,631,828,213</td>
    </tr>
    <tr>
      <td>Map output bytes</td>
      <td>7,667,457,405</td>
      <td>0</td>
      <td>7,667,457,405</td>
    </tr>
    <tr>
      <td>Total committed heap usage (bytes)</td>
      <td>1,038,745,600</td>
      <td>29,097,984</td>
      <td>1,067,843,584</td>
    </tr>
    <tr>
      <td>CPU time spent (ms)</td>
      <td>2,957,800</td>
      <td>2,104,030</td>
      <td>5,061,830</td>
    </tr>
    <tr>
      <td>Virtual memory (bytes) snapshot</td>
      <td>4,112,838,656</td>
      <td>1,380,306,944</td>
      <td>5,493,145,600</td>
    </tr>
    <tr>
      <td>SPLIT_RAW_BYTES</td>
      <td>384</td>
      <td>0</td>
      <td>384</td>
    </tr>
    <tr>
      <td><strong>Map output records</strong></td>
      <td><strong><em>426,010,418</em></strong></td>
      <td>0</td>
      <td>426,010,418</td>
    </tr>
    <tr>
      <td><strong>Combine input records</strong></td>
      <td><strong><em>851,296,316</em></strong></td>
      <td>0</td>
      <td>851,296,316</td>
    </tr>
    <tr>
      <td>Reduce input records</td>
      <td>0</td>
      <td>401,113,702</td>
      <td>401,113,702</td>
    </tr>
  </tbody>
</table>

<p>From which we can see, specially metrics which highlighted in bold, that in the job I only pass in about 7MB data file with 1000 lines of records, but Reducer outputs 381,696,888 records, which are 2.1G  B compressed gz file and some 9GB plain text when decompressed.</p>

<p>But clearly it’s not the problem of my code that leads to so much disk space usage, the above output metrics are all reasonable, although you may be surprised by the comparison between 7MB with only 1000 records input and  9GB  with 381,696,888 records output. The truth is that I’m calculating co-appearance combination computation.</p>

<p>From this experimental I learned that my personal computer really cannot play with big elephant, input data records from the first 10 thousand to 5 thousand to 3 thousand to the last ONE thousand, but data analytic should go on, I need to find a solution to work it out, actually I have 30 times of data need to process, that is 30 thousand records.</p>

<p>Yet still have a lot of work to do, and I plan to post some articles about what’s I have done with my <em>big data</em> :) and <em>Hadoop</em> so far.</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
</feed>
