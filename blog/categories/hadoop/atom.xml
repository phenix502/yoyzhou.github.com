<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Tony Chou's Blog]]></title>
  <link href="http://yoyzhou.github.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://yoyzhou.github.com/"/>
  <updated>2013-06-04T23:11:52+08:00</updated>
  <id>http://yoyzhou.github.com/</id>
  <author>
    <name><![CDATA[yoyzhou]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[让Mahout KMeans聚类分析运行在Hadoop上]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/06/04/mahout-clustering-with-hadoop/"/>
    <updated>2013-06-04T20:18:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/06/04/mahout-clustering-with-hadoop</id>
    <content type="html"><![CDATA[<p>上一篇文章<a href="/blog/2013/05/26/clustering-with-mahout/">“Mahout与聚类分析”</a>介绍了如何使用Mahout进行聚类分析的步骤，并且结合实例使用K-Means对微博名人共同关注数据进行了共被关注聚类分析。Mahout运行有本地运行和Hadoop运行两种模式，本地运行是指在用户本地的单机模式下运行，就像运行其他普通的程序一样，但是这样这样就不能最大限度的发挥出Mahout的优势，在本文中我们介绍如何让我们的Mahout聚类分析程序在Hahoop集群上运行（在实际操作中笔者使用的伪分布Hadoop，而不是真正的Hadoop集群）。</p>

<h4 id="mahout">配置Mahout运行环境</h4>

<p>Mahout运行配置可以在<code>$MAHOUT_HOME/bin/mahout</code>里面进行设置，实际上<code>$MAHOUT_HOME/bin/mahout</code>就是Mahout在命令行的启动脚本，这一点与Hadoop相似，但也又不同，Hadoop在$HADOOP_HOME\conf下面还提供了专门的hadoop-env.sh文件进行相关环境变量的配置，而Mahout在conf目录下没有提供这样的文件。</p>

<h5 id="mahoutlocalhadoopconfdir">MAHOUT_LOCAL与HADOOP_CONF_DIR</h5>

<p>以上的连个参数是控制Mahout是在本地运行还是在Hadoop上运行的关键。</p>

<p><code>$MAHOUT_HOME/bin/mahout</code>文件指出，<strong>只要设置<code>MAHOUT_LOCAL</code>的值为一个非空（not empty string）值，则不管用户有没有设置HADOOP_CONF_DIR和HADOOP_HOME这两个参数，Mahout都以本地模式运行</strong>；换句话说，如果要想Mahout运行在Hadoop上，则MAHOUT_LOCAL必须为空。 </p>

<p><code>HADOOP_CONF_DIR</code>参数指定Mahout运行Hadoop模式时使用的Hadoop配置信息，这个文件目录一般指向的是$HADOOP_HOME目录下的conf目录。</p>

<p>除此之外，我们还应该设置<code>JAVA_HOME</code>或者<code>MAHOUT_JAVA_HOME</code>变量，以及必须将Hadoop的执行文件加入到PATH中。</p>

<p>综上所述：</p>

<p>1. 添加<code>JAVA_HOME</code>变量，可以在直接设置在<code>$MAHOUT_HOME/bin/mahout</code>中，也可以在user/bash profile里面设置(如<code>./bashrc</code>)</p>

<p>2. 设置MAHOUT_HOME并添加Hadoop的执行文件到PATH中</p>

<p>两个步骤在<code>~/.bashrc</code>的设置如下：</p>

<pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
#export HADOOP_HOME=/home/yoyzhou/workspace/hadoop-1.1.2
export MAHOUT_HOME=/home/yoyzhou/workspace/mahout-0.7
export PATH=$PATH:/home/yoyzhou/workspace/hadoop-1.1.2/bin:$MAHOUT_HOME/bin
</code></pre>

<p>编辑完<code>~/.bashrc</code>,重启Terminal即可生效。</p>

<p>3. 编辑<code>$MAHOUT_HOME/bin/mahout</code>，将<code>HADOOP_CONF_DIR</code>设置为<code>$HADOOP_HOME\conf</code></p>

<pre><code>HADOOP_CONF_DIR=/home/yoyzhou/workspace/hadoop-1.1.2/conf
</code></pre>

<p>读者可以将相关的Hadoop和Mahout主目录修改自己系统上面的目录地址，设置好之后重启Terminal，在命令行输入mahout，如果你看到如下的信息，就说明Mahout的Hadoop运行模式已经配置好了。</p>

<pre><code>MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath. 
Running on hadoop...
</code></pre>

<p>要想使用本地模式运行，只需在<code>$MAHOUT_HOME/bin/mahout</code>添加一条设置<code>MAHOUT_LOCAL</code>为非空的语句即可。</p>

<h4 id="mahout-1">Mahout命令行</h4>

<p>Mahout为相关的数据挖掘算法提供了相应的命令行入口，同时提供了一些数据分析处理的用到的工具集。这些命令可以通过在终端输入<code>mahout</code>获得。以下显示了输入<code>mahout</code>的部分信息：</p>

<pre><code>....
Valid program names are:
  arff.vector: : Generate Vectors from an ARFF file or directory
  baumwelch: : Baum-Welch algorithm for unsupervised HMM training
  canopy: : Canopy clustering
  cat: : Print a file or resource as the logistic regression models would see it
  cleansvd: : Cleanup and verification of SVD output
  clusterdump: : Dump cluster output to text
  ....
  fkmeans: : Fuzzy K-means clustering
  fpg: : Frequent Pattern Growth
  hmmpredict: : Generate random sequence of observations by given HMM
  itemsimilarity: : Compute the item-item-similarities for item-based collaborative filtering
  kmeans: : K-means clustering
....
</code></pre>

<h4 id="mahout-kmeans">Mahout kmeans</h4>

<p>在上一篇<a href="/blog/2013/05/26/clustering-with-mahout/">文章</a>，我们通过调用KMeansDriver.run()方法从Mahout程序中直接启动KMeans算法，这种方式对于在本地调试程序非常有用，但是在真实项目中，无论是使用Hadoop模式运行，还是本地运行，从命令行运行Mahout的相关算法更加合适，这样的好处是我们只需要给Mahout提供符合相应算法要求的输入数据，即可以利用Mahout分布式处理的优势。比如在本例中，使用kmeans算法，只需要事先将数据处理成Mahout kmeans算法要求的输入数据，然后在命令行调用<code>mahout kmeans [options]</code>即可。</p>

<p>在命令行输入不带任何参数的<code>mahout kmeans</code>，Mahout将为你列出在命令行使用kmeans算法的使用方法。</p>

<pre><code>Usage:                                                                          
 [--input &lt;input&gt; --output &lt;output&gt; --distanceMeasure &lt;distanceMeasure&gt;         
--clusters &lt;clusters&gt; --numClusters &lt;k&gt; --convergenceDelta &lt;convergenceDelta&gt;   
--maxIter &lt;maxIter&gt; --overwrite --clustering --method &lt;method&gt;                  
--outlierThreshold &lt;outlierThreshold&gt; --help --tempDir &lt;tempDir&gt; --startPhase   
&lt;startPhase&gt; --endPhase &lt;endPhase&gt;]                                             
--clusters (-c) clusters    The input centroids, as Vectors.  Must be a         
	                        SequenceFile of Writable, Cluster/Canopy.  If k is  
	                        also specified, then a random set of vectors will   
	                        be selected and written out to this path first 
</code></pre>

<p>相关的参数我们已经在上篇<a href="/blog/2013/05/26/clustering-with-mahout/">文章</a>中提到过。</p>

<p>具体的步骤如下：</p>

<pre><code>1. 将数据处理为Mahout向量（Vector）的形式
2. 将Mahout向量转化为Hadoop SequenceFile
3. 创建K个初始质心\[可选\]
4. 将Mahout向量的SequenceFile复制到HDFS上
5. 运行`mahout kmeans [options]`
</code></pre>

<p>下面的命令显示使用CosineDistanceMeasure对data/vectors目录下Mahout向量数据进行kmeans聚类分析，输出结果保存在output目录下。</p>

<pre><code>mahout kmeans -i data/vectors -o output -c data/clusters \
-dm org.apache.mahout.common.distance.CosineDistanceMeasure \
-x 10 -ow -cd 0.001 -cl
</code></pre>

<p>更加详细的命令行参数可以在Mahout wiki <a href="https://cwiki.apache.org/MAHOUT/k-means-commandline.html">k-means-commandline</a>上查找到。</p>

<h4 id="section">总结</h4>

<p>本文首先介绍了如何配置Mahout的Hadoop的运行环境，然后介绍如何使用mahout kmeans命令行将聚类分析运行在Hadoop上。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mahout与聚类分析]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/05/26/clustering-with-mahout/"/>
    <updated>2013-05-26T21:53:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/05/26/clustering-with-mahout</id>
    <content type="html"><![CDATA[<p>本文简要的介绍了Mahout以及聚类分析，并结合实例说明了如何使用Mahout的K-Means算法进行聚类分析。</p>

<h4 id="mahout">Mahout</h4>

<p>Mahout是Apache下的开源机器学习软件包，目前实现的机器学习算法主要包含有<strong>协同过滤/推荐引擎</strong>，<strong>聚类</strong>和<strong>分类</strong>三个部分。Mahout从设计开始就旨在建立可扩展的机器学习软件包，用于处理大数据机器学习的问题，当你正在研究的数据量大到不能在一台机器上运行时，就可以选择使用Mahout，让你的数据在Hadoop集群的进行分析。Mahout某些部分的实现直接创建在Hadoop之上，这就使得其具有进行大数据处理的能力，也是Mahout最大的优势所在。相比较于<a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>，<a href="https://rapid-i.com/content/view/181/190/">RapidMiner</a>等图形化的机器学习软件，Mahout只提供机器学习的程序包（library），不提供用户图形界面，并且Mahout并不包含所有的机器学习算法实现，这一点可以算得上是她的一个劣势，但前面提到过Mahout并不是“又一个机器学习软件”，而是要成为一个“可扩展的用于处理大数据的机器学习软件”，但是我相信会有越来越多的机器学习算法会在Mahout上面实现。</p>

<h4 id="section">聚类分析</h4>

<p>物以类聚，人以群分。顾名思义，聚类分析就是将不同的对象分为不同的组或簇，它与我们日常生活所理解的类的概念是相一致的。聚类分析能够帮助我们很好地了解对象之间的类与结构，聚类分析也能够帮助我们将个别对象指派到相应的类。</p>

<blockquote>
  <p>聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的（相关的），而不同组之间的对象是不同的（不相关的）。组内的相似性（同质性）越大，组间差别越大，聚类就越好。<br />《数据挖掘导论》 Pang-Ning Tan等</p>
</blockquote>

<p>常见的聚类分析有K-Means聚类和层次聚类两种。此外还有基于密度的、基于图的以及基于模型的聚类分析方法。</p>

<p>从上面的定义中可以看出，聚类的分析的关键之一在于相似性的度量，常用的相似性有基于距离的相似度，余玄相似度，Jaccard相似度以及Pearson相关系数等。</p>

<h4 id="mahout-1">Mahout聚类分析</h4>

<p>Mahout的聚类分析提供了K-Means聚类、Fuzzy K-Means聚类和基于模型的聚类等方法。下面我们以K-Means聚类来说明如何使用Mahout进行聚类分析。</p>

<h5 id="section-1">1. 数据准备</h5>

<p>在进行数据分析时，第一步必然是准备分析数据。根据研究或者项目的要求收集数据，并且对数据进行必要的预处理。比如我下面我们要提到的微博用户共被关注分析中，首先我们要收集微博用户的关注信息，然后从关注信息中提取出用户共被关注的数据，也就是进行数据预处理。最终我们需要的数据可能就像下面的格式这样直白明确：“用户1,用户2,共被关注次数”。如果你是要进行文档的聚类分析，那么首先需要的获得的是文档的TF-IDF向量（文档-词向量）。</p>

<h5 id="mahoutvector">2. 将数据转换成Mahout中的Vector</h5>

<p>Mahout要求每一个输入数据都应该是一个Vector，并且以Hadoop的SequenceFile类型存储。简单来说N维<strong>Vector</strong>就是一条N维<strong>记录</strong>，其中<strong>维</strong>就是记录的<strong>属性</strong>。比如一个TF-IDF向量就是一个<strong>文档</strong>记录，里面的属性就是<strong>词</strong>，有n个词就有n个维。有时候属性也叫<strong>特征</strong>。</p>

<p>在准备好Vectors之后，需要将这些记录写入到Hadoop Sequence文件中。下面的代码片段演示和如何创建Mahout Vector和将记录写成Sequence文件。</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//…create Vector</span>
</span><span class='line'><span class="c1">//NamedVector is a Vector Wrapper class which owns a name for each vector, very convenient class </span>
</span><span class='line'><span class="n">NamedVector</span> <span class="n">vec</span> <span class="o">=</span> <span class="k">new</span> <span class="n">NamedVector</span><span class="o">(</span><span class="k">new</span> <span class="n">RandomAccessSparseVector</span><span class="o">(</span><span class="n">cflist</span><span class="o">.</span><span class="na">size</span><span class="o">()</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="n">account</span><span class="o">);</span>
</span><span class='line'><span class="n">Iterator</span><span class="o">&lt;</span><span class="n">cofollowednode</span><span class="o">&gt;</span> <span class="n">nodeItr</span> <span class="o">=</span> <span class="n">cflist</span><span class="o">.</span><span class="na">iterator</span><span class="o">();</span>
</span><span class='line'><span class="k">while</span><span class="o">(</span><span class="n">nodeItr</span><span class="o">.</span><span class="na">hasNext</span><span class="o">()){</span>
</span><span class='line'>	<span class="n">CoFollowedNode</span> <span class="n">cfnode</span><span class="o">=</span> <span class="n">nodeItr</span><span class="o">.</span><span class="na">next</span><span class="o">();</span>
</span><span class='line'>	<span class="c1">//set vector&#39;s n-dimension with co-followed number</span>
</span><span class='line'>	<span class="n">vec</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">cfnode</span><span class="o">.</span><span class="na">getDemension</span><span class="o">(),</span> <span class="n">cfnode</span><span class="o">.</span><span class="na">getCoFollowedNum</span><span class="o">());</span>
</span><span class='line'><span class="o">}&lt;/</span><span class="n">cofollowednode</span><span class="o">&gt;&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="cm">/**</span>
</span><span class='line'><span class="cm">  * Write Vectors to Hadoop sequence file, it’s required per Mahout implementation that Vectors passed</span>
</span><span class='line'><span class="cm">  * into Mahout clusterings must be in Hadoop sequence file. </span>
</span><span class='line'><span class="cm">  * </span>
</span><span class='line'><span class="cm">  **/</span>
</span><span class='line'><span class="kd">private</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">writeVectorsToSeqFile</span><span class="o">(</span><span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">namedvector</span><span class="o">&gt;</span> <span class="n">vectors</span><span class="o">,</span> <span class="n">String</span> <span class="n">filename</span><span class="o">,</span>
</span><span class='line'>		<span class="n">FileSystem</span> <span class="n">fs</span><span class="o">,</span> <span class="n">Configuration</span> <span class="n">conf</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{&lt;/</span><span class="n">namedvector</span><span class="o">&gt;&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">filename</span><span class="o">);</span>
</span><span class='line'><span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span> <span class="n">writer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">conf</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span>
</span><span class='line'>			<span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">VectorWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>	
</span><span class='line'><span class="n">VectorWritable</span> <span class="n">vec</span> <span class="o">=</span> <span class="k">new</span> <span class="n">VectorWritable</span><span class="o">();</span>
</span><span class='line'>	
</span><span class='line'><span class="c1">//write vectors to file, the key is the account id, value is the co-followed vector of the key</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">NamedVector</span> <span class="n">entry</span> <span class="o">:</span> <span class="n">vectors</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>	<span class="n">vec</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">entry</span><span class="o">);</span>
</span><span class='line'>	<span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">entry</span><span class="o">.</span><span class="na">getName</span><span class="o">()),</span> <span class="n">vec</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h5 id="mahoutk-means">3. 使用Mahout中的K-Means进行聚类</h5>

<p>在介绍如何使用Mahout的K-Means之前，先复习一下基本的K-Means算法，如下所述：</p>

<pre><code>**基本的K-Means算法**

1, 选择K个点作为初始质心

2, repeat:

3,	将每个点指派到最近的质心，形成K个簇

4,	重新计算每个簇的质心

5, until 质心不再发生改变
</code></pre>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//write initial clusters point, in this case 5 points/vectors</span>
</span><span class='line'><span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">clusters</span><span class="o">/</span><span class="n">part</span><span class="o">-</span><span class="mi">00000</span><span class="err">”</span><span class="o">);</span>
</span><span class='line'><span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span> <span class="n">writer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">conf</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span>
</span><span class='line'>		<span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Kluster</span><span class="o">.</span><span class="na">class</span><span class="o">);&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span> <span class="n">k</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>	<span class="n">NamedVector</span> <span class="n">nVec</span> <span class="o">=</span>  <span class="n">vectors</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">);&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Kluster</span> <span class="n">cluster</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Kluster</span><span class="o">(</span><span class="n">nVec</span><span class="o">,</span> <span class="n">i</span><span class="o">,</span> <span class="k">new</span> <span class="n">CosineDistanceMeasure</span><span class="o">());</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">cluster</span><span class="o">.</span><span class="na">getIdentifier</span><span class="o">()),</span> <span class="n">cluster</span><span class="o">);</span> <span class="o">}</span> <span class="n">writer</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//here runs the k-means clustering in mapreduce mode, set runSequential to false</span>
</span><span class='line'><span class="n">KMeansDriver</span><span class="o">.</span><span class="na">run</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">vectors</span><span class="err">”</span><span class="o">),</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span>
</span><span class='line'>		<span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">clusters</span><span class="err">”</span><span class="o">),</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">output</span><span class="err">”</span><span class="o">),</span>
</span><span class='line'>		<span class="k">new</span> <span class="nf">CosineDistanceMeasure</span><span class="o">(),</span> <span class="mf">0.001</span><span class="o">,</span> <span class="mi">10</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>上面的代码显示了Mahout中运行K-Means算法的用法，首先提供K个初始质心，如果用户没有提供初始质心，Mahout将根据用户给出的K随机的选择K个点作为初始质心。</p>

<p>KMeansDriver提供了运行K-Means算法的入口，其中的参数包含：</p>

<pre><code>/**
   	* Iterate over the input vectors to produce clusters and, if requested, use the results of the final iteration to
   	* cluster the input vectors.
   	* 
   	* @param input
   	*          the directory pathname for input points
   	* @param clustersIn
   	*          the directory pathname for initial &amp; computed clusters
   	* @param output
   	*          the directory pathname for output points
   	* @param measure
   	*          the DistanceMeasure to use
   	* @param convergenceDelta
   	*          the convergence delta value
   	* @param maxIterations
   	*          the maximum number of iterations
   	* @param runClustering
   	*          true if points are to be clustered after iterations are completed
   	* @param clusterClassificationThreshold
   	*          Is a clustering strictness / outlier removal parameter. Its value should be between 0 and 1. Vectors
   	*          having pdf below this value will not be clustered.
   	* @param runSequential
   	*          if true execute sequential algorithm, else run algorithm in mapreduce mode 
   	*/
</code></pre>

<h4 id="section-2">总结</h4>

<p>本文简要的介绍了Mahout以及聚类分析，并使用实例说明了如何使用Mahout的K-Means进行聚类分析。总的来说，使用Mahout进行聚类分析需要用户将数据转换成Mahout的Vector对象，并且写成Sequence文件格式，然后选择初始的质心和适当的距离度量调用KMeansDriver进行聚类。</p>

<p>当然聚类分析还有很多的内容，比如如何选择距离度量，K值如何确定以及如何度量聚类的好坏等等，本篇文章旨在介绍如何使用Mahout进行聚类分析，更多的关于聚类以及机器学习的知识请您阅读相关的专业书籍。</p>

<h4 id="section-3">参考数目</h4>

<p>[1] Sean Owen etc., Mahout in Action, Manning Publications, 2011</p>

<p>[2] Pang-Ning Tan等, 数据挖掘导论, 人民邮电出版社, 2011 </p>

<p>[3] 文中代码实例的源码地址<a href="https://github.com/yoyzhou/weibo_analysis/blob/master/src/com/yoyzhou/weibo/CoFollowedClustering.java">CoFollowedClustering.java</a></p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adding Filter in Hadoop Mapper Class]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/21/adding-filter-in-hadoop-mapper-class/"/>
    <updated>2013-04-21T15:08:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/21/adding-filter-in-hadoop-mapper-class</id>
    <content type="html"><![CDATA[<p>There is my solutions to tackle the disk spaces shortage problem I described in the <a href="/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results/">previous post</a>. The core principle of the solution is to reduce the number of output records at Mapper stage; the method I used is <strong>Filter</strong>, adding a filter, which I will explain later, to decrease the output records of Mapper, which in turn significantly decrease the Mapper’s Spill records, and fundamentally decrease the disk space usages. After applying the filter, with 30,661 records. some 200MB data set as inputs, the total Spill Records is 25,471,725,  and it <strong>only</strong> takes about 509MB disk spaces!</p>

<h4 id="followed-filter">Followed Filter</h4>

<p>And now I’m going to reveal what’s kinda Filter it looks like, and how did I accomplish that filter.
The true face of the <strong>FILTER</strong> is called  <strong>Followed Filter</strong>, it filters users from computing co-followed combinations if their followed number does not satisfy a certain number, called <strong>Followed Threshold</strong>. </p>

<p>Followed Filter is used to reduce the co-followed combinations at Mapper stage. Say we set the followed threshold to 100, meaning users who doesn’t own 100 fans(be followed by 100 other users) will be ignored during co-followed combinations computing stage(to get the actual number of the threshold we need analyze statistics of user’s followed number of our data set).</p>

<h4 id="reason">Reason</h4>

<p>Choosing followed filter is reasonable because how many user follows is a metric of user’s popularity/famousness.</p>

<h4 id="how">HOW</h4>

<p>In order to accomplish it, we need:</p>

<p><strong>First</strong>, counting user’s followed number among our data set, which needs a new MapReduce Job;</p>

<p><strong>Second</strong>, choosing a followed threshold after analyze the statistics perspective of followed number data set got in first step;</p>

<p><strong>Third</strong>, using DistrbutedCache of Hadoop to cache users who satisfy the filter to all Mappers;</p>

<p><strong>Forth</strong>, adding followed filter to Mapper class, only users satisfy filter condition will be passed into co-followed combination computing phrase;</p>

<p><strong>Fifth</strong>, adding co-followed filter/threshold in Reducer side if necessary.</p>

<h4 id="outcomes">Outcomes</h4>
<p>Here is the Hadoop Job Summary, after applying the followed filter with followed threshold of 1000, that means only users who are followed by 1000 users will have the opportunity to co-followed combinations, compared with the Job Summary in my previous post, most all metrics have significant improvements:</p>

<table>
  <tbody>
    <tr>
      <td>Counter</td>
      <td>Map</td>
      <td>Reduce</td>
      <td>Total</td>
    </tr>
    <tr>
      <td>Bytes Written</td>
      <td>0</td>
      <td>1,798,185</td>
      <td>1,798,185</td>
    </tr>
    <tr>
      <td>Bytes Read</td>
      <td>203,401,876</td>
      <td>0</td>
      <td>203,401,876</td>
    </tr>
    <tr>
      <td>FILE_BYTES_READ</td>
      <td>405,219,906</td>
      <td>52,107,486</td>
      <td>457,327,392</td>
    </tr>
    <tr>
      <td><strong><em>HDFS_BYTES_READ</em></strong></td>
      <td><strong><em>203,402,751</em></strong></td>
      <td>0</td>
      <td>203,402,751</td>
    </tr>
    <tr>
      <td><strong><em>FILE_BYTES_WRITTEN</em></strong></td>
      <td>457,707,759</td>
      <td>52,161,704</td>
      <td><strong><em>509,869,463</em></strong></td>
    </tr>
    <tr>
      <td>HDFS_BYTES_WRITTEN</td>
      <td>0</td>
      <td>1,798,185</td>
      <td>1,798,185</td>
    </tr>
    <tr>
      <td>Reduce input groups</td>
      <td>0</td>
      <td>373,680</td>
      <td>373,680</td>
    </tr>
    <tr>
      <td>Map output materialized bytes</td>
      <td>52,107,522</td>
      <td>0</td>
      <td>52,107,522</td>
    </tr>
    <tr>
      <td>Combine output records</td>
      <td>22,202,756</td>
      <td>0</td>
      <td>22,202,756</td>
    </tr>
    <tr>
      <td><strong><em>Map input records</em></strong></td>
      <td><strong><em>30,661</em></strong></td>
      <td>0</td>
      <td>30,661</td>
    </tr>
    <tr>
      <td>Reduce shuffle bytes</td>
      <td>0</td>
      <td>52,107,522</td>
      <td>52,107,522</td>
    </tr>
    <tr>
      <td>Physical memory (bytes) snapshot</td>
      <td>2,646,589,440</td>
      <td>116,408,320</td>
      <td>2,762,997,760</td>
    </tr>
    <tr>
      <td><strong><em>Reduce output records</em></strong></td>
      <td>0</td>
      <td><strong><em>373,680</em></strong></td>
      <td>373,680</td>
    </tr>
    <tr>
      <td><strong><em>Spilled Records</em></strong></td>
      <td><strong><em>22,866,351</em></strong></td>
      <td>2,605,374</td>
      <td>25,471,725</td>
    </tr>
    <tr>
      <td>Map output bytes</td>
      <td>2,115,139,050</td>
      <td>0</td>
      <td>2,115,139,050</td>
    </tr>
    <tr>
      <td>Total committed heap usage (bytes)</td>
      <td>2,813,853,696</td>
      <td>84,738,048</td>
      <td>2,898,591,744</td>
    </tr>
    <tr>
      <td>CPU time spent (ms)</td>
      <td>5,766,680</td>
      <td>11,210</td>
      <td>5,777,890</td>
    </tr>
    <tr>
      <td>Virtual memory (bytes) snapshot</td>
      <td>9,600,737,280</td>
      <td>1,375,002,624</td>
      <td>10,975,739,904</td>
    </tr>
    <tr>
      <td>SPLIT_RAW_BYTES</td>
      <td>875</td>
      <td>0</td>
      <td>875</td>
    </tr>
    <tr>
      <td><strong><em>Map output records</em></strong></td>
      <td><strong><em>117,507,725</em></strong></td>
      <td>0</td>
      <td>117,507,725</td>
    </tr>
    <tr>
      <td>Combine input records</td>
      <td>137,105,107</td>
      <td>0</td>
      <td>137,105,107</td>
    </tr>
    <tr>
      <td>Reduce input records</td>
      <td>0</td>
      <td>2,605,374</td>
      <td>2,605,374</td>
    </tr>
  </tbody>
</table>

<h4 id="ps">P.S.</h4>
<p>Frankly Speaking, chances are <strong>I am on the wrong way to Hadoop Programming</strong>, since I’m palying <code>Pesudo Distribution Hadoop</code> with my personal computer, which has 4 CUPs and 4G RAM, in real Hadoop Cluster disk spaces might never be a trouble, and all the tuning work I have done may turn into meaningless efforts. Before the Followed Filter, I also did some Hadoop tuning like customed Writable class, RawComparator, block size and io.sort.mb, etc.</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My First Lucky and Sad Hadoop Results]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results/"/>
    <updated>2013-04-20T17:05:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results</id>
    <content type="html"><![CDATA[<p>Recently I am playing with Hadoop per analyzing the data set I scraped from WEIBO.COM. After a couple of tryings, many are failed due to disk space shortage, after I decreased the input date set volumn, luckily I gained a completed Hadoop Job results, but, sadly, with only 1000 lines of records processed.</p>

<p>Here is the Job Summary:</p>

<table>
  <tbody>
    <tr>
      <td>Counter</td>
      <td>Map</td>
      <td>Reduce</td>
      <td>Total</td>
    </tr>
    <tr>
      <td>Bytes Read</td>
      <td>7,945,196</td>
      <td>0</td>
      <td>7,945,196</td>
    </tr>
    <tr>
      <td>FILE_BYTES_READ</td>
      <td>16,590,565,518</td>
      <td>8,021,579,181</td>
      <td>24,612,144,699</td>
    </tr>
    <tr>
      <td><strong>HDFS_BYTES_READ</strong></td>
      <td><strong><em>7,945,580</em></strong></td>
      <td>0</td>
      <td>7,945,580</td>
    </tr>
    <tr>
      <td>FILE_BYTES_WRITTEN</td>
      <td>24,612,303,774</td>
      <td>8,021,632,091</td>
      <td>32,633,935,865</td>
    </tr>
    <tr>
      <td>HDFS_BYTES_WRITTEN</td>
      <td>0</td>
      <td>2,054,409,494</td>
      <td>2,054,409,494</td>
    </tr>
    <tr>
      <td>Reduce input groups</td>
      <td>0</td>
      <td>381,696,888</td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Map output materialized bytes</td>
      <td>8,021,579,181</td>
      <td>0</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Combine output records</td>
      <td>826,399,600</td>
      <td>0</td>
      <td>826,399,600</td>
    </tr>
    <tr>
      <td><strong>Map input records</strong></td>
      <td><strong><em>1,000</em></strong></td>
      <td>0</td>
      <td>1,000</td>
    </tr>
    <tr>
      <td>Reduce shuffle bytes</td>
      <td>0</td>
      <td>8,021,579,181</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Physical memory (bytes) snapshot</td>
      <td>1,215,041,536</td>
      <td>72,613,888</td>
      <td>1,287,655,424</td>
    </tr>
    <tr>
      <td><strong>Reduce output records</strong></td>
      <td>0</td>
      <td><strong><em>381,696,888</em></strong></td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Spilled Records</td>
      <td>1,230,714,511</td>
      <td>401,113,702</td>
      <td>1,631,828,213</td>
    </tr>
    <tr>
      <td>Map output bytes</td>
      <td>7,667,457,405</td>
      <td>0</td>
      <td>7,667,457,405</td>
    </tr>
    <tr>
      <td>Total committed heap usage (bytes)</td>
      <td>1,038,745,600</td>
      <td>29,097,984</td>
      <td>1,067,843,584</td>
    </tr>
    <tr>
      <td>CPU time spent (ms)</td>
      <td>2,957,800</td>
      <td>2,104,030</td>
      <td>5,061,830</td>
    </tr>
    <tr>
      <td>Virtual memory (bytes) snapshot</td>
      <td>4,112,838,656</td>
      <td>1,380,306,944</td>
      <td>5,493,145,600</td>
    </tr>
    <tr>
      <td>SPLIT_RAW_BYTES</td>
      <td>384</td>
      <td>0</td>
      <td>384</td>
    </tr>
    <tr>
      <td><strong>Map output records</strong></td>
      <td><strong><em>426,010,418</em></strong></td>
      <td>0</td>
      <td>426,010,418</td>
    </tr>
    <tr>
      <td><strong>Combine input records</strong></td>
      <td><strong><em>851,296,316</em></strong></td>
      <td>0</td>
      <td>851,296,316</td>
    </tr>
    <tr>
      <td>Reduce input records</td>
      <td>0</td>
      <td>401,113,702</td>
      <td>401,113,702</td>
    </tr>
  </tbody>
</table>

<p>From which we can see that, specially metrics which highlighted in bold style, I only passed in about 7MB data file with 1000 lines of records, but Reducer outputs 381,696,888 records, which are 2.1GB compressed gz file and some 9GB plain text when decompressed.</p>

<p>But clearly it’s not the problem of my code that leads to so much disk space usages, the above output metrics are all reasonable, although you may be surprised by the comparison between 7MB with only 1000 records input and  9GB  with 381,696,888 records output. The truth is that I’m calculating co-appearance combination computation.</p>

<p>From this experimental I learned that my personal computer really cannot play with big elephant, input data records from the first 10 thousand down to 5 thousand to 3 thousand to ONE thousand at last, but data analytic should go on, I need to find a solution to work it out, actually I have 30 times of data need to process, that is 30 thousand records.</p>

<p>Yet still have a lot of work to do, and I plan to post some articles about what’s I have done with my <em>big data</em> :) and <em>Hadoop</em> so far.</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Eclipse开发MapReduce程序的步骤]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse/"/>
    <updated>2013-04-14T20:58:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse</id>
    <content type="html"><![CDATA[<p>以下8个步骤是我在使用Eclipse开发MapReduce程序时的路线，假定读者已经配置好了Hadoop环境并且了解Eclipse的相关操作。</p>

<p>步骤<code>0～4</code>为在Eclipse中编写和调试MapReduce程序；步骤<code>5、6</code>为在伪分布模式下运行MapReduce程序，并且通过导出项目到指定目录实现了Eclipse项目与Hadoop的关联。</p>

<p>0 创建Java项目</p>

<p>1 在项目的CLASS PATH中添加<a href="http://hadoop.apache.org/">Hadoop</a>相关的JAR引用（注意在添加JAR文件，而不是JAR文件夹，要不然在4中会因为找不到JAR或者Class而报错）</p>

<blockquote>
  <p>如果你还下载了Hadoop的<a href="http://hadoop.apache.org/releases.html">源码</a>，也可以给Hadoop相关的JAR添加源码，这样在Eclipse就可以使用F3参看Hadoop源码）</p>
</blockquote>

<p>2 按照<a href="http://hadoop.apache.org/docs/r1.0.4/mapred_tutorial.html">MapReduce</a>类规范，编写自己的MapReduce类</p>

<p>3 配置MapReduce类的运行参数</p>

<p>4 在Eclipse中以单机模式运行/调试程序</p>

<p>5 将程序导出（Export）为JAR文件到$HADOOP_HOME/lib下</p>

<p>6 在伪分布模式下运行程序 bin/hadoop jar lib/ur-exported-jar.JAR full-class-name 参数列表</p>

<blockquote>
  <p>例如，你导出的JAR文件名为myhadoop.jar，类名称com.coolcompany.wordcount，命令就是：bin/hadoop jar lib/myhadoop.jar com.coolcompany.wordcount 参数列表</p>
</blockquote>

<p>7 部署程序到真实的Hadoop集群</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
</feed>
