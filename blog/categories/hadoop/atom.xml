<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Tony Chou's Blog]]></title>
  <link href="http://yoyzhou.github.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://yoyzhou.github.com/"/>
  <updated>2013-05-27T21:27:31+08:00</updated>
  <id>http://yoyzhou.github.com/</id>
  <author>
    <name><![CDATA[yoyzhou]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mahout与聚类分析]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/05/26/clustering-with-mahout/"/>
    <updated>2013-05-26T21:53:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/05/26/clustering-with-mahout</id>
    <content type="html"><![CDATA[<p>本文简要的介绍了Mahout以及聚类分析，并结合实例说明了如何使用Mahout的K-Means算法进行聚类分析。</p>

<h4 id="mahout">Mahout</h4>

<p>Mahout是Apache下的开源机器学习软件包，目前实现的机器学习算法主要包含有<strong>协同过滤/推荐引擎</strong>，<strong>聚类</strong>和<strong>分类</strong>三个部分。Mahout从设计开始就旨在建立可扩展的机器学习软件包，用于处理大数据机器学习的问题，当你正在研究的数据量大到不能在一台机器上运行时，就可以选择使用Mahout，让你的数据在Hadoop集群的进行分析。Mahout某些部分的实现直接创建在Hadoop之上，这就使得其具有进行大数据处理的能力，也是Mahout最大的优势所在。相比较于Weka，Fast-Miner等图形化的机器学习软件，Mahout只提供机器学习的程序包（library），不提供用户图形界面，并且Mahout并不包含所有的机器学习算法实现，这一点可以算得上是她的一个劣势，但前面提到过Mahout并不是“又一个机器学习软件”，而是要成为一个“可扩展的用于处理大数据的机器学习软件”，但是我相信会有越来越多的机器学习算法会在Mahout上面实现。</p>

<h4 id="section">聚类分析</h4>

<p>物以类聚，人以群分。顾名思义，聚类分析就是将不同的对象分为不同的组或簇，它与我们日常生活所理解的类的概念是相一致的。聚类分析能够帮助我们很好地了解对象之间的类与结构，聚类分析也能够帮助我们将个别对象指派到相应的类。</p>

<blockquote>
  <p>聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的（相关的），而不同组之间的对象是不同的（不相关的）。组内的相似性（同质性）越大，组间差别越大，聚类就越好。<br />《数据挖掘导论》 Pang-Ning Tan等</p>
</blockquote>

<p>常见的聚类分析有K-Means聚类和层次聚类两种。此外还有基于密度的、基于图的以及基于模型的聚类分析方法。</p>

<p>从上面的定义中可以看出，聚类的分析的关键之一在于相似性的度量，常用的相似性有基于距离的相似度，余玄相似度，Jaccard相似度以及Pearson相关系数等。</p>

<h4 id="mahout-1">Mahout聚类分析</h4>

<p>Mahout的聚类分析提供了K-Means聚类、Fuzzy K-Means聚类和基于模型的聚类等方法。下面我们以K-Means聚类来说明如何使用Mahout进行聚类分析。</p>

<h5 id="section-1">1. 数据准备</h5>

<p>在进行数据分析时，第一步必然是准备分析数据。根据研究或者项目的要求收集数据，并且对数据进行必要的预处理。比如我下面我们要提到的微博用户共被关注分析中，首先我们要收集微博用户的关注信息，然后从关注信息中提取出用户共被关注的数据，也就是进行数据预处理。最终我们需要的数据可能就像下面的格式这样直白明确：“用户1,用户2,共被关注次数”。如果你是要进行文档的聚类分析，那么首先需要的获得的是文档的TF-IDF向量（文档-词向量）。</p>

<h5 id="mahoutvector">2. 将数据转换成Mahout中的Vector</h5>

<p>Mahout要求每一个输入数据都应该是一个Vector，并且以Hadoop的SequenceFile类型存储。简单来说N维<strong>Vector</strong>就是一条N维<strong>记录</strong>，其中<strong>维</strong>就是记录的<strong>属性</strong>。比如一个TF-IDF向量就是一个<strong>文档</strong>记录，里面的属性就是<strong>词</strong>，有n个词就有n个维。有时候属性也叫<strong>特征</strong>。</p>

<p>在准备好Vectors之后，需要将这些记录写入到Hadoop Sequence文件中。下面的代码片段演示和如何创建Mahout Vector和将记录写成Sequence文件。</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//…create Vector</span>
</span><span class='line'><span class="c1">//NamedVector is a Vector Wrapper class which owns a name for each vector, very convenient class </span>
</span><span class='line'><span class="n">NamedVector</span> <span class="n">vec</span> <span class="o">=</span> <span class="k">new</span> <span class="n">NamedVector</span><span class="o">(</span><span class="k">new</span> <span class="n">RandomAccessSparseVector</span><span class="o">(</span><span class="n">cflist</span><span class="o">.</span><span class="na">size</span><span class="o">()</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="n">account</span><span class="o">);</span>
</span><span class='line'><span class="n">Iterator</span><span class="o">&lt;</span><span class="n">cofollowednode</span><span class="o">&gt;</span> <span class="n">nodeItr</span> <span class="o">=</span> <span class="n">cflist</span><span class="o">.</span><span class="na">iterator</span><span class="o">();</span>
</span><span class='line'><span class="k">while</span><span class="o">(</span><span class="n">nodeItr</span><span class="o">.</span><span class="na">hasNext</span><span class="o">()){</span>
</span><span class='line'>	<span class="n">CoFollowedNode</span> <span class="n">cfnode</span><span class="o">=</span> <span class="n">nodeItr</span><span class="o">.</span><span class="na">next</span><span class="o">();</span>
</span><span class='line'>	<span class="c1">//set vector&#39;s n-dimension with co-followed number</span>
</span><span class='line'>	<span class="n">vec</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">cfnode</span><span class="o">.</span><span class="na">getDemension</span><span class="o">(),</span> <span class="n">cfnode</span><span class="o">.</span><span class="na">getCoFollowedNum</span><span class="o">());</span>
</span><span class='line'><span class="o">}&lt;/</span><span class="n">cofollowednode</span><span class="o">&gt;&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="cm">/**</span>
</span><span class='line'><span class="cm">  * Write Vectors to Hadoop sequence file, it’s required per Mahout implementation that Vectors passed</span>
</span><span class='line'><span class="cm">  * into Mahout clusterings must be in Hadoop sequence file. </span>
</span><span class='line'><span class="cm">  * </span>
</span><span class='line'><span class="cm">  **/</span>
</span><span class='line'><span class="kd">private</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">writeVectorsToSeqFile</span><span class="o">(</span><span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">namedvector</span><span class="o">&gt;</span> <span class="n">vectors</span><span class="o">,</span> <span class="n">String</span> <span class="n">filename</span><span class="o">,</span>
</span><span class='line'>		<span class="n">FileSystem</span> <span class="n">fs</span><span class="o">,</span> <span class="n">Configuration</span> <span class="n">conf</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{&lt;/</span><span class="n">namedvector</span><span class="o">&gt;&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">filename</span><span class="o">);</span>
</span><span class='line'><span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span> <span class="n">writer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">conf</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span>
</span><span class='line'>			<span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">VectorWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>	
</span><span class='line'><span class="n">VectorWritable</span> <span class="n">vec</span> <span class="o">=</span> <span class="k">new</span> <span class="n">VectorWritable</span><span class="o">();</span>
</span><span class='line'>	
</span><span class='line'><span class="c1">//write vectors to file, the key is the account id, value is the co-followed vector of the key</span>
</span><span class='line'><span class="k">for</span> <span class="o">(</span><span class="n">NamedVector</span> <span class="n">entry</span> <span class="o">:</span> <span class="n">vectors</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>	<span class="n">vec</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">entry</span><span class="o">);</span>
</span><span class='line'>	<span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">entry</span><span class="o">.</span><span class="na">getName</span><span class="o">()),</span> <span class="n">vec</span><span class="o">);</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h5 id="mahoutk-means">3. 使用Mahout中的K-Means进行聚类</h5>

<p>在介绍如何使用Mahout的K-Means之前，先复习一下基本的K-Means算法，如下所述：</p>

<pre><code>**基本的K-Means算法**

1, 选择K个点作为初始质心

2, repeat:

3,	将每个点指派到最近的质心，形成K个簇

4,	重新计算每个簇的质心

5, until 质心不再发生改变
</code></pre>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//write initial clusters point, in this case 5 points/vectors</span>
</span><span class='line'><span class="n">Path</span> <span class="n">path</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">clusters</span><span class="o">/</span><span class="n">part</span><span class="o">-</span><span class="mi">00000</span><span class="err">”</span><span class="o">);</span>
</span><span class='line'><span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span> <span class="n">writer</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFile</span><span class="o">.</span><span class="na">Writer</span><span class="o">(</span><span class="n">fs</span><span class="o">,</span> <span class="n">conf</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span>
</span><span class='line'>		<span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Kluster</span><span class="o">.</span><span class="na">class</span><span class="o">);&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span> <span class="n">k</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
</span><span class='line'>	<span class="n">NamedVector</span> <span class="n">nVec</span> <span class="o">=</span>  <span class="n">vectors</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">i</span><span class="o">);&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">Kluster</span> <span class="n">cluster</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Kluster</span><span class="o">(</span><span class="n">nVec</span><span class="o">,</span> <span class="n">i</span><span class="o">,</span> <span class="k">new</span> <span class="n">CosineDistanceMeasure</span><span class="o">());</span>
</span><span class='line'><span class="n">writer</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="k">new</span> <span class="n">Text</span><span class="o">(</span><span class="n">cluster</span><span class="o">.</span><span class="na">getIdentifier</span><span class="o">()),</span> <span class="n">cluster</span><span class="o">);</span> <span class="o">}</span> <span class="n">writer</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="c1">//here runs the k-means clustering in mapreduce mode, set runSequential to false</span>
</span><span class='line'><span class="n">KMeansDriver</span><span class="o">.</span><span class="na">run</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">vectors</span><span class="err">”</span><span class="o">),</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span>
</span><span class='line'>		<span class="err">“</span><span class="n">data</span><span class="o">/</span><span class="n">clusters</span><span class="err">”</span><span class="o">),</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="err">“</span><span class="n">output</span><span class="err">”</span><span class="o">),</span>
</span><span class='line'>		<span class="k">new</span> <span class="nf">CosineDistanceMeasure</span><span class="o">(),</span> <span class="mf">0.001</span><span class="o">,</span> <span class="mi">10</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>上面的代码显示了Mahout中运行K-Means算法的用法，首先提供K个初始质心，如果用户没有提供初始质心，Mahout将根据用户给出的K随机的选择K个点作为初始质心。</p>

<p>KMeansDriver提供了运行K-Means算法的入口，其中的参数包含：</p>

<pre><code>/**
   	* Iterate over the input vectors to produce clusters and, if requested, use the results of the final iteration to
   	* cluster the input vectors.
   	* 
   	* @param input
   	*          the directory pathname for input points
   	* @param clustersIn
   	*          the directory pathname for initial &amp; computed clusters
   	* @param output
   	*          the directory pathname for output points
   	* @param measure
   	*          the DistanceMeasure to use
   	* @param convergenceDelta
   	*          the convergence delta value
   	* @param maxIterations
   	*          the maximum number of iterations
   	* @param runClustering
   	*          true if points are to be clustered after iterations are completed
   	* @param clusterClassificationThreshold
   	*          Is a clustering strictness / outlier removal parameter. Its value should be between 0 and 1. Vectors
   	*          having pdf below this value will not be clustered.
   	* @param runSequential
   	*          if true execute sequential algorithm, else run algorithm in mapreduce mode 
   	*/
</code></pre>

<h4 id="section-2">总结</h4>

<p>本文简要的介绍了Mahout以及聚类分析，并使用实例说明了如何使用Mahout的K-Means进行聚类分析。总的来说，使用Mahout进行聚类分析需要用户将数据转换成Mahout的Vector对象，并且写成Sequence文件格式，然后选择初始的质心和适当的距离度量调用KMeansDriver进行聚类。</p>

<p>当然聚类分析还有很多的内容，比如如何选择距离度量，K值如何确定以及如何度量聚类的好坏等等，本篇文章旨在介绍如何使用Mahout进行聚类分析，更多的关于聚类以及机器学习的知识请您阅读相关的专业书籍。</p>

<h4 id="section-3">参考数目</h4>

<p>[1] Sean Owen etc., Mahout in Action, Manning Publications, 2011</p>

<p>[2] Pang-Ning Tan等, 数据挖掘导论, 人民邮电出版社, 2011 </p>

<p>[3] 文中代码实例的源码地址<a href="https://github.com/yoyzhou/weibo_analysis/blob/master/src/com/yoyzhou/weibo/CoFollowedClustering.java">CoFollowedClustering.java</a></p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adding Filter in Hadoop Mapper Class]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/21/adding-filter-in-hadoop-mapper-class/"/>
    <updated>2013-04-21T15:08:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/21/adding-filter-in-hadoop-mapper-class</id>
    <content type="html"><![CDATA[<p>There is my solutions to tackle the disk spaces shortage problem I described in the <a href="/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results/">previous post</a>. The core principle of the solution is to reduce the number of output records at Mapper stage; the method I used is <strong>Filter</strong>, adding a filter, which I will explain later, to decrease the output records of Mapper, which in turn significantly decrease the Mapper’s Spill records, and fundamentally decrease the disk space usages. After applying the filter, with 30,661 records. some 200MB data set as inputs, the total Spill Records is 25,471,725,  and it <strong>only</strong> takes about 509MB disk spaces!</p>

<h4 id="followed-filter">Followed Filter</h4>

<p>And now I’m going to reveal what’s kinda Filter it looks like, and how did I accomplish that filter.
The true face of the <strong>FILTER</strong> is called  <strong>Followed Filter</strong>, it filters users from computing co-followed combinations if their followed number does not satisfy a certain number, called <strong>Followed Threshold</strong>. </p>

<p>Followed Filter is used to reduce the co-followed combinations at Mapper stage. Say we set the followed threshold to 100, meaning users who doesn’t own 100 fans(be followed by 100 other users) will be ignored during co-followed combinations computing stage(to get the actual number of the threshold we need analyze statistics of user’s followed number of our data set).</p>

<h4 id="reason">Reason</h4>

<p>Choosing followed filter is reasonable because how many user follows is a metric of user’s popularity/famousness.</p>

<h4 id="how">HOW</h4>

<p>In order to accomplish it, we need:</p>

<p><strong>First</strong>, counting user’s followed number among our data set, which needs a new MapReduce Job;</p>

<p><strong>Second</strong>, choosing a followed threshold after analyze the statistics perspective of followed number data set got in first step;</p>

<p><strong>Third</strong>, using DistrbutedCache of Hadoop to cache users who satisfy the filter to all Mappers;</p>

<p><strong>Forth</strong>, adding followed filter to Mapper class, only users satisfy filter condition will be passed into co-followed combination computing phrase;</p>

<p><strong>Fifth</strong>, adding co-followed filter/threshold in Reducer side if necessary.</p>

<h4 id="outcomes">Outcomes</h4>
<p>Here is the Hadoop Job Summary, after applying the followed filter with followed threshold of 1000, that means only users who are followed by 1000 users will have the opportunity to co-followed combinations, compared with the Job Summary in my previous post, most all metrics have significant improvements:</p>

<table>
  <tbody>
    <tr>
      <td>Counter</td>
      <td>Map</td>
      <td>Reduce</td>
      <td>Total</td>
    </tr>
    <tr>
      <td>Bytes Written</td>
      <td>0</td>
      <td>1,798,185</td>
      <td>1,798,185</td>
    </tr>
    <tr>
      <td>Bytes Read</td>
      <td>203,401,876</td>
      <td>0</td>
      <td>203,401,876</td>
    </tr>
    <tr>
      <td>FILE_BYTES_READ</td>
      <td>405,219,906</td>
      <td>52,107,486</td>
      <td>457,327,392</td>
    </tr>
    <tr>
      <td><strong><em>HDFS_BYTES_READ</em></strong></td>
      <td><strong><em>203,402,751</em></strong></td>
      <td>0</td>
      <td>203,402,751</td>
    </tr>
    <tr>
      <td><strong><em>FILE_BYTES_WRITTEN</em></strong></td>
      <td>457,707,759</td>
      <td>52,161,704</td>
      <td><strong><em>509,869,463</em></strong></td>
    </tr>
    <tr>
      <td>HDFS_BYTES_WRITTEN</td>
      <td>0</td>
      <td>1,798,185</td>
      <td>1,798,185</td>
    </tr>
    <tr>
      <td>Reduce input groups</td>
      <td>0</td>
      <td>373,680</td>
      <td>373,680</td>
    </tr>
    <tr>
      <td>Map output materialized bytes</td>
      <td>52,107,522</td>
      <td>0</td>
      <td>52,107,522</td>
    </tr>
    <tr>
      <td>Combine output records</td>
      <td>22,202,756</td>
      <td>0</td>
      <td>22,202,756</td>
    </tr>
    <tr>
      <td><strong><em>Map input records</em></strong></td>
      <td><strong><em>30,661</em></strong></td>
      <td>0</td>
      <td>30,661</td>
    </tr>
    <tr>
      <td>Reduce shuffle bytes</td>
      <td>0</td>
      <td>52,107,522</td>
      <td>52,107,522</td>
    </tr>
    <tr>
      <td>Physical memory (bytes) snapshot</td>
      <td>2,646,589,440</td>
      <td>116,408,320</td>
      <td>2,762,997,760</td>
    </tr>
    <tr>
      <td><strong><em>Reduce output records</em></strong></td>
      <td>0</td>
      <td><strong><em>373,680</em></strong></td>
      <td>373,680</td>
    </tr>
    <tr>
      <td><strong><em>Spilled Records</em></strong></td>
      <td><strong><em>22,866,351</em></strong></td>
      <td>2,605,374</td>
      <td>25,471,725</td>
    </tr>
    <tr>
      <td>Map output bytes</td>
      <td>2,115,139,050</td>
      <td>0</td>
      <td>2,115,139,050</td>
    </tr>
    <tr>
      <td>Total committed heap usage (bytes)</td>
      <td>2,813,853,696</td>
      <td>84,738,048</td>
      <td>2,898,591,744</td>
    </tr>
    <tr>
      <td>CPU time spent (ms)</td>
      <td>5,766,680</td>
      <td>11,210</td>
      <td>5,777,890</td>
    </tr>
    <tr>
      <td>Virtual memory (bytes) snapshot</td>
      <td>9,600,737,280</td>
      <td>1,375,002,624</td>
      <td>10,975,739,904</td>
    </tr>
    <tr>
      <td>SPLIT_RAW_BYTES</td>
      <td>875</td>
      <td>0</td>
      <td>875</td>
    </tr>
    <tr>
      <td><strong><em>Map output records</em></strong></td>
      <td><strong><em>117,507,725</em></strong></td>
      <td>0</td>
      <td>117,507,725</td>
    </tr>
    <tr>
      <td>Combine input records</td>
      <td>137,105,107</td>
      <td>0</td>
      <td>137,105,107</td>
    </tr>
    <tr>
      <td>Reduce input records</td>
      <td>0</td>
      <td>2,605,374</td>
      <td>2,605,374</td>
    </tr>
  </tbody>
</table>

<h4 id="ps">P.S.</h4>
<p>Frankly Speaking, chances are <strong>I am on the wrong way to Hadoop Programming</strong>, since I’m palying <code>Pesudo Distribution Hadoop</code> with my personal computer, which has 4 CUPs and 4G RAM, in real Hadoop Cluster disk spaces might never be a trouble, and all the tuning work I have done may turn into meaningless efforts. Before the Followed Filter, I also did some Hadoop tuning like customed Writable class, RawComparator, block size and io.sort.mb, etc.</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My First Lucky and Sad Hadoop Results]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results/"/>
    <updated>2013-04-20T17:05:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/20/my-first-luckily-and-sad-hadoop-results</id>
    <content type="html"><![CDATA[<p>Recently I am playing with Hadoop per analyzing the data set I scraped from WEIBO.COM. After a couple of tryings, many are failed due to disk space shortage, after I decreased the input date set volumn, luckily I gained a completed Hadoop Job results, but, sadly, with only 1000 lines of records processed.</p>

<p>Here is the Job Summary:</p>

<table>
  <tbody>
    <tr>
      <td>Counter</td>
      <td>Map</td>
      <td>Reduce</td>
      <td>Total</td>
    </tr>
    <tr>
      <td>Bytes Read</td>
      <td>7,945,196</td>
      <td>0</td>
      <td>7,945,196</td>
    </tr>
    <tr>
      <td>FILE_BYTES_READ</td>
      <td>16,590,565,518</td>
      <td>8,021,579,181</td>
      <td>24,612,144,699</td>
    </tr>
    <tr>
      <td><strong>HDFS_BYTES_READ</strong></td>
      <td><strong><em>7,945,580</em></strong></td>
      <td>0</td>
      <td>7,945,580</td>
    </tr>
    <tr>
      <td>FILE_BYTES_WRITTEN</td>
      <td>24,612,303,774</td>
      <td>8,021,632,091</td>
      <td>32,633,935,865</td>
    </tr>
    <tr>
      <td>HDFS_BYTES_WRITTEN</td>
      <td>0</td>
      <td>2,054,409,494</td>
      <td>2,054,409,494</td>
    </tr>
    <tr>
      <td>Reduce input groups</td>
      <td>0</td>
      <td>381,696,888</td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Map output materialized bytes</td>
      <td>8,021,579,181</td>
      <td>0</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Combine output records</td>
      <td>826,399,600</td>
      <td>0</td>
      <td>826,399,600</td>
    </tr>
    <tr>
      <td><strong>Map input records</strong></td>
      <td><strong><em>1,000</em></strong></td>
      <td>0</td>
      <td>1,000</td>
    </tr>
    <tr>
      <td>Reduce shuffle bytes</td>
      <td>0</td>
      <td>8,021,579,181</td>
      <td>8,021,579,181</td>
    </tr>
    <tr>
      <td>Physical memory (bytes) snapshot</td>
      <td>1,215,041,536</td>
      <td>72,613,888</td>
      <td>1,287,655,424</td>
    </tr>
    <tr>
      <td><strong>Reduce output records</strong></td>
      <td>0</td>
      <td><strong><em>381,696,888</em></strong></td>
      <td>381,696,888</td>
    </tr>
    <tr>
      <td>Spilled Records</td>
      <td>1,230,714,511</td>
      <td>401,113,702</td>
      <td>1,631,828,213</td>
    </tr>
    <tr>
      <td>Map output bytes</td>
      <td>7,667,457,405</td>
      <td>0</td>
      <td>7,667,457,405</td>
    </tr>
    <tr>
      <td>Total committed heap usage (bytes)</td>
      <td>1,038,745,600</td>
      <td>29,097,984</td>
      <td>1,067,843,584</td>
    </tr>
    <tr>
      <td>CPU time spent (ms)</td>
      <td>2,957,800</td>
      <td>2,104,030</td>
      <td>5,061,830</td>
    </tr>
    <tr>
      <td>Virtual memory (bytes) snapshot</td>
      <td>4,112,838,656</td>
      <td>1,380,306,944</td>
      <td>5,493,145,600</td>
    </tr>
    <tr>
      <td>SPLIT_RAW_BYTES</td>
      <td>384</td>
      <td>0</td>
      <td>384</td>
    </tr>
    <tr>
      <td><strong>Map output records</strong></td>
      <td><strong><em>426,010,418</em></strong></td>
      <td>0</td>
      <td>426,010,418</td>
    </tr>
    <tr>
      <td><strong>Combine input records</strong></td>
      <td><strong><em>851,296,316</em></strong></td>
      <td>0</td>
      <td>851,296,316</td>
    </tr>
    <tr>
      <td>Reduce input records</td>
      <td>0</td>
      <td>401,113,702</td>
      <td>401,113,702</td>
    </tr>
  </tbody>
</table>

<p>From which we can see that, specially metrics which highlighted in bold style, I only passed in about 7MB data file with 1000 lines of records, but Reducer outputs 381,696,888 records, which are 2.1GB compressed gz file and some 9GB plain text when decompressed.</p>

<p>But clearly it’s not the problem of my code that leads to so much disk space usages, the above output metrics are all reasonable, although you may be surprised by the comparison between 7MB with only 1000 records input and  9GB  with 381,696,888 records output. The truth is that I’m calculating co-appearance combination computation.</p>

<p>From this experimental I learned that my personal computer really cannot play with big elephant, input data records from the first 10 thousand down to 5 thousand to 3 thousand to ONE thousand at last, but data analytic should go on, I need to find a solution to work it out, actually I have 30 times of data need to process, that is 30 thousand records.</p>

<p>Yet still have a lot of work to do, and I plan to post some articles about what’s I have done with my <em>big data</em> :) and <em>Hadoop</em> so far.</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Eclipse开发MapReduce程序的步骤]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse/"/>
    <updated>2013-04-14T20:58:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse</id>
    <content type="html"><![CDATA[<p>以下8个步骤是我在使用Eclipse开发MapReduce程序时的路线，假定读者已经配置好了Hadoop环境并且了解Eclipse的相关操作。</p>

<p>步骤<code>0～4</code>为在Eclipse中编写和调试MapReduce程序；步骤<code>5、6</code>为在伪分布模式下运行MapReduce程序，并且通过导出项目到指定目录实现了Eclipse项目与Hadoop的关联。</p>

<p>0 创建Java项目</p>

<p>1 在项目的CLASS PATH中添加<a href="http://hadoop.apache.org/">Hadoop</a>相关的JAR引用（注意在添加JAR文件，而不是JAR文件夹，要不然在4中会因为找不到JAR或者Class而报错）</p>

<blockquote>
  <p>如果你还下载了Hadoop的<a href="http://hadoop.apache.org/releases.html">源码</a>，也可以给Hadoop相关的JAR添加源码，这样在Eclipse就可以使用F3参看Hadoop源码）</p>
</blockquote>

<p>2 按照<a href="http://hadoop.apache.org/docs/r1.0.4/mapred_tutorial.html">MapReduce</a>类规范，编写自己的MapReduce类</p>

<p>3 配置MapReduce类的运行参数</p>

<p>4 在Eclipse中以单机模式运行/调试程序</p>

<p>5 将程序导出（Export）为JAR文件到$HADOOP_HOME/lib下</p>

<p>6 在伪分布模式下运行程序 bin/hadoop jar lib/ur-exported-jar.JAR full-class-name 参数列表</p>

<blockquote>
  <p>例如，你导出的JAR文件名为myhadoop.jar，类名称com.coolcompany.wordcount，命令就是：bin/hadoop jar lib/myhadoop.jar com.coolcompany.wordcount 参数列表</p>
</blockquote>

<p>7 部署程序到真实的Hadoop集群</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop in Action学习笔记]]></title>
    <link href="http://yoyzhou.github.com/blog/2013/04/14/hadoop-in-action-reading-note/"/>
    <updated>2013-04-14T15:41:00+08:00</updated>
    <id>http://yoyzhou.github.com/blog/2013/04/14/hadoop-in-action-reading-note</id>
    <content type="html"><![CDATA[<h4 id="hadoop">第一章 Hadoop简介</h4>

<p>现今，互联网每天都产生海量的数据，现有工具对于TB、PB级别大规模分布式海量数据变得无力处理。</p>

<p>Google首先推出了处理大规模分布式数据的MapReduce计算范式，Doug Cutting领导开发了一个开源版的MapReduce，后来成为Hadoop。</p>

<h5 id="hadoop-1">什么是Hadoop</h5>

<p>Hadoop是一个开源框架，可编写和运行分布式应用处理大规模数据。一般认为Hadoop包含两个核心部分：HDFS，Hadoop分布式文件系统和MapReduce，一种分布式编程范式（分布式存储和分布式计算）。</p>

<pre><code>**优点：**
• 方便
• 健壮
• 线性可扩展
• 简单
</code></pre>

<h5 id="vs-">分布式系统  VS 大型单机服务器</h5>

<pre><code>向外扩展 VS  向上扩展

构建在一般/低端商用机器上，廉价

高端服务器，价格昂贵
</code></pre>

<h5 id="hadoopsetihome">Hadoop设计理念和SETI@Home的区别</h5>

<p>Hadoop设计用于数据密集型任务，遵循将程序向数据移动的设计哲学，即尽可能的保持数据不动，减少I/O的访问，程序文件的数量级相对于数据的数量级小得多</p>

<p>SETI@Home设计用于计算密集型任务，遵循将数据向计算资源（计算机）移动的设计哲学；因为数据传输量小，但是需要大量的计算资源（CPU时间）</p>

<h5 id="hadoopsql">Hadoop与SQL数据库的比较</h5>
<p>都是处理数据，SQL数据库用于处理结构化数据</p>

<p>Hadoop应用针对的是文本数据/可能是结构的也可能使非结构或者半结构化数据</p>

<p>1. 向外扩展 代替向上扩展</p>

<p>2. 用键/值对 代替关系表</p>

<p>3. 用函数式编程（MapReduce）代替申明式查询（SQL）</p>

<p>4. 用离线处理代替在线处理</p>

<p>数据密集型分布式应用中，数据传输的代价昂贵，应保持数据存储和处理紧密的绑定在一起。</p>

<h5 id="hadoop-2">Hadoop的历史</h5>
<p>2004年 Goole发表Google文件系统（GFS）和MapReduce框架</p>

<p>Doug Cutting将Nutch项目移植到GFS和MapReduce上，并设立了专门的项目充实这两种技术，于是就有了Hadoop</p>

<p>2006年 Yahoo聘用Doug Cutting，让他和一个专门团队一起改进Hadoop</p>

<p>2008年 Hahoop成为Apache的顶级项目</p>

<h4 id="hadoop-3">第二章 初识Hadoop</h4>

<p><strong>“运行Hadoop”意味着在不同服务器运行一组守护进程（daemons）:</strong></p>

<pre><code>0 NameNode (名字节点)
1 DataNode (数据节点)
2 Secondary NameNode (次名字节点)
3 JobTracker (作业跟踪节点)
4 TaskTracker (任务跟踪节点)
</code></pre>

<p>Hadoop在分布式计算和分布式存储中都采用了<strong>主/从(Master/Slaver)</strong>的结构</p>

<p>NameNode，JobTracker是Master节点</p>

<p>DataNode，TaskTracker和Slaver节点</p>

<h5 id="hadoop-4">运行Hadoop</h5>
<p>配置文件：core-site.xml, hdfs-site.xml和mapred-site.xml</p>

<p>conf/masters - master节点主机列表</p>

<p>conf/slaves - slave节点主机列表</p>

<p>运行的三种模式：本地（单机）模式，伪分布模式和全分布模式</p>

<pre><code>bin/hadoop namenode -format
bin/start-all.sh
jps
</code></pre>

<h5 id="web">基于WEB的管理界面</h5>
<p>NameNode状态界面： namenode-host:50070</p>

<p>JobTracker状态界面： jobtacker-host:50030</p>

<h4 id="hadoop-5">第三章 Hadoop组件</h4>

<p><strong>HDFS文件系统</strong></p>

<p>HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式处理而设计的</p>

<p>可以处理单个的大数据集（比如100TB），而大多数文件系统物理实现这点。</p>

<p><strong>？</strong>是不是说HDFS处理单个大数据集更加有效，而处理小文件的大量数据变现的不是很有优势，当然，如果这样也可以先把小文件组成大文件。</p>

<p><strong>？</strong>文件要多大才能适合HDFS/Hadoop的处理呢?</p>

<p>可不要为了数据大而大，数据应该在必要的清洗和预处理</p>

<h5 id="hadoop-6">Hadoop的基本文件命令</h5>

<p>hadoop fs -cmd &lt;args&gt;</p>

<p>HDFS中，默认的根目录是/user/$USER 其中USER是你登录系统的用户名</p>

<p>1. 添加文件和目录</p>

<p><code>hadoop fs -mkdir &lt;dir-name&gt;</code></p>

<p>将在/user/$USER目录下创建&lt;dir-name&gt;，如果&lt;dir-name&gt;包含多层目录且上层目录不存在，hadoop fs -mkdir会创建指定的多层目录结构。</p>

<p>如:</p>

<p><code>hadoop fs -mkdir inputs/dataset1 会创建/user/$USER/inputs/dataset1</code></p>

<p>2. 列举文件清单</p>

<p><code>hadoop fs -ls &lt;file-dir-name&gt;</code></p>

<p>和Unix系统ls本地文件类似</p>

<p><code>hadoop fs -lsr &lt;file-dir-name&gt;</code></p>

<p>查看所有文件和文件的子目录，递归的列出文件清单</p>

<p>3. 复制本地文件到HDFS </p>

<p><code>hadoop fs -put &lt;file-dir-name-to-put-in-hdfs&gt; &lt;file-dir-on-hdfs&gt;</code></p>

<p>4. 从HDFS上检索文件到本地系统</p>

<p><code>hadoop fs -get &lt;file-dir-on-hdfs-to-get&gt; &lt;file-dir-name-on-localhost&gt;</code></p>

<p>这是一个与hadoop fs -put相反的操作。</p>

<p>5. 查看HDFS上文件的内容</p>

<p><code>hadoop fs -cat &lt;file-name-to-cat&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -cat example.txt</code></p>

<p>同样假设文件在默认的工作目录/user/$USER下</p>

<p>6. 删除文件/目录</p>

<p><code>hadoop fs -rm &lt;file-name-to-remove&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -rm example.txt</code></p>

<p>这个命令用来删除文件，要删除文件夹是使用 -rmr 命令：</p>

<p><code>hadoop fs -rmr &lt;dir-name-to-remove&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -rmr input</code></p>

<p>这里的<code>r</code>代表递归的删除文件。</p>

<p>7. 查看文件命令帮助</p>

<p><code>hadoop fs -help &lt;cmd&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -help rmr</code></p>

<p>查看rmr命令的帮助文件</p>

<p>8. hadoop文件命令与Unix管道一起使用</p>

<p>例如：</p>

<p><code>hadoop fs -cat example.txt | head -n 20</code></p>

<h5 id="hadoop-7">Hadoop数据的读和写</h5>

<p>遵循并行处理数据分片原则的输入数据通常为单一的大文件。这也是Hadoop分布式文件系统的设计策略。</p>

<p>FSDataInputStream支持随机读取，这一特性涉及到MapReduce的数据分片机制。</p>

<p>随机读取，当一个任务失败时，恢复时不用从头读取文件，只需要从失败的位置进行恢复。</p>

<p><strong>InputFormat</strong></p>

<p>Hadoop分割与读取输入文件的方式在InputFormat接口中定义，TextInputFormat是InputFormat的默认实现。Hadoop提供的其他的InputFormat实现有：</p>

<p>KeyValueTextInputFormat，SequenceFileInputFormat和NLineInputFormat</p>

<p><strong>OutputFormat</strong></p>

<p>通过使用IntWritable类变量可以提高reduce()的性能，IntWritable类的处理性能要比Text高。</p>

<h4 id="mapreduce">第四章 编写MapReduce基础程序</h4>

<p><strong>mapper container partitioner reducer</strong> </p>

<h5 id="combiner">使用Combiner提升性能</h5>

<p>Combiner作用上与Reduce等价，起到聚合、结合作用，原则上程序必须没有Combiner也能够得到正确的结果</p>

<p>1. 网络洗牌时减少数据传输流量，考虑10亿条Mapper键值对的输出，如果没有Combiner会在网络洗牌过程中造成多大的流量</p>

<p>2. 数据分布不均匀时，造成大量Mapper的键值对输出跑向同一个Reducer</p>

<p>原理就是通过Combiner减少Mapper的输出数量以降低网络和Reducer上的压力，Combiner位于Mapper和Reducer中间，被视为是Reducer的助手，在数据转换上必须与Reducer等价，也就是如果我们去掉Combiner，Reducer的输出应该保持不变。</p>

<p>但是Combiner未必会提高性能，这要看Combiner是否能有效的减少Mapper输出记录的数量。</p>

<h4 id="mapreduce-1">第五章 高阶MapReduce</h4>

<h5 id="mapreduce-2">MapReduce之间的依赖</h5>

<p>Hadoop通过Job和JobControl类来管理作业之间的非线性依赖关系。</p>

<p>x.addDependingJob(y)</p>

<p>ChainMapper ChainReducer</p>

<p>链接MapReduce是，mapper、reducer之间的输入输出按照值传递还是引用传递的问题。P100</p>

<p>如果确保上游的Map/Reduce不是用其输出数据，或者下游Map/Reduce不改变上游的输出数据，可以使用by reference以提高一定的性能</p>

<h5 id="section">联结不同来源的数据</h5>

<p>1. Reduce侧的联结</p>

<p>repartitioned join 重分区联结</p>

<p>repartitioned sort-mergejoin 重分区排序-合并联结</p>

<p>结合DataJoin包使用钩子处理数据流P93</p>

<p>2. Mapper侧的联结P98</p>

<p><strong>DistributedCache 分布式缓存</strong></p>

<p>应用场景：一个数据源较大，另一个数据源可能小几个数量级，将较小的数据源装入内存，复制到所有的Mapper，在map阶段执行联结。通常小的数据源也叫做“背景数据”</p>

<h4 id="section-1">第六章 编程实践</h4>

<h5 id="section-2">本地模式</h5>

<p><strong>1 计算的完整性检查</strong></p>

<p>数学和逻辑错误在数据密集型程序中更加普遍，而它们往往并不明显！</p>

<p>数学计数或者算术如何检查正确性，在分布式计算中数学公式可能要重新设计，但是如何验证计算的完整性和准确性也就十分的重要。</p>

<p><strong>方法：</strong></p>

<p>可是通过宏观的查看一些指标，比如平均值，整体计数、最大值等来进行初步验证</p>

<p><strong>2 回归测试</strong></p>

<p>可能代价会很大，但是可以选取一部分数据集进行测试。所谓回归测试就是在同一数据集上比较代码修改前后算法的输出结果是否一致，如果每次回归测试的结果都是一样的，那么可以判定修改没有导致出现新的问题。</p>

<p><strong>3 考虑使用LONG而不是INT</strong></p>

<h5 id="section-3">伪分布模式</h5>
<p>1 日志</p>

<p>2 JOBTRACKER的WEB管理界面</p>

<p>3 杀掉作业</p>

<h5 id="section-4">生产集群上的监控与调试</h5>
<p>计数器 Reporter.incrCounter()</p>

<p>跳过坏记录 skipping模式的设置P126</p>

<p>用IsolationRunner重新运行出错的任务</p>

<h5 id="section-5">性能调优</h5>

<p><strong>Hadoop的线性可扩展性</strong></p>

<h6 id="combiner-1">1 通过combiner来减少网络流量</h6>

<h6 id="section-6">2 减少输入数据</h6>
<pre><code>a. 采样数据，只处理数据的子集
b. “重构”数据，仅使用需要的字段（要求开发人员清楚的了解数据和自己的业务需求）
#上面两个方案一个是**横切**一个是**纵切**，都是减少输入数据的方法
</code></pre>

<h6 id="section-7">3 使用压缩</h6>

<p>即使使用了combiner，在map阶段的输出也可能很大。这些中间数据必须被存储在磁盘上，并在网络上重排。压缩这些中间数据会提高大多数MapReduce作业的性能。Hadoop内置支持压缩和解压缩。</p>

<pre><code>压缩的参数配置P130
mapred.compress.map.output
mapred.map.output.compression.codec
SequenceFIleOutputFormat 序列文件输出
</code></pre>

<h6 id="jvm">4 重用JVM</h6>

<p>mapred.job.reuse.jvm.num.task 指定一个JVM可以运行的最大任务数</p>

<h6 id="section-8">5 根据猜测执行来运行</h6>

<p>在所有的mapper完成之前，reducer都不会启动；类似的，在所有的reducer完成之前，一个作业也不会结束。</p>

<p><strong>问题：当其中一个任务变慢时（注意不是失效），MapReduce如何处理？</strong></p>

<p>Hadoop会注意到运行速度缓慢的任务，并安排在另一个节点上并行执行相同的任务。</p>

<pre><code>**参数：**

mapred.map.tasks.speculative.execution
mapred.reduce.tasks.speculative.execution
</code></pre>

<h6 id="section-9">6 代码重构与算法重写</h6>

<p>6.1 将Streaming程序重写为Java程序</p>

<p>6.2 在一次作业中集中一次处理类似的任务，而不是每一个任务都启动一个作业，比如计算最大值、最小值和均值等在同一数据集上的计算可以在一个Job中完成，而不要分为不同的Job</p>

<p>6.3 设计特定的新的适合MapReduce框架的算法</p>

<h4 id="section-10">第七章 细则手册</h4>

<h5 id="section-11">7.1 向任务传递作业定制的参数</h5>

<p>在JobConf中定制自己的参数，在所有的Task中都能读取到。</p>

<h5 id="section-12">7.2 探查任务特定信息</h5>

<h5 id="section-13">7.3 划分为多个输出文件</h5>
<p>MultipleOutputFormat 键/值区分，写入不同的文件， 一个Collector 不同文件名</p>

<p>MultipleOutputs	属性区分，写入不同的文件，多个Collector写文件</p>

<h5 id="section-14">7.4 以数据库作为输入输出</h5>
<p><strong>DBOutputFormat</strong></p>

<h5 id="section-15">7.5 保持输出的顺序</h5>

<h4 id="hadoop-8">第八章 管理Hadoop</h4>

<h5 id="section-16">8.1 为实际应用设置特定参数值</h5>

<h5 id="section-17">8.2 系统体检</h5>
<pre><code>bin/hadoop fsck &lt;path&gt; #file system check
bin/hadoop dfsadmin -report
</code></pre>

<h5 id="section-18">8.3 权限设置</h5>

<h5 id="section-19">8.4 配额管理</h5>
<pre><code>bin/hadoop dfsadmin -setQuota &lt;N&gt; dir
bin/hadoop dfsadmin -setSpaceQuota &lt;N&gt; dir
bin/hadoop fs -count -q dir #显示目录的配额
</code></pre>

<h5 id="section-20">8.5 启动回收站</h5>

<p>fs.trash.interval属性（以分钟为单位）</p>

<h5 id="datanode">8.6 删减DataNode</h5>

<pre><code>dfs.hosts.exclude=file-point-to-exclude-host-list
bin/hadoop dfsadmin -refreshNodes
</code></pre>

<h5 id="datanode-1">8.7 增加DataNode</h5>

<h5 id="namenodesnn">8.8 管理NameNode和SNN</h5>
<p>减轻NameNode负担的一种方法是增加数据块的大小，以降低文件系统中元数据的数据量，dfs.block.size 默认64M</p>

<h6 id="snn">SNN是做什么的？</h6>
<p>取了一个不妥的名字。首先并不是NameNode的失效备份</p>

<p>把SNN视为一个检查点服务器更为合适，用于合并下面两个文件以形成系统快照。</p>

<pre><code>可能在0.21中被弃用
FsImage
EditLog
</code></pre>

<h5 id="namenode">8.9 恢复失效的NameNode</h5>
<p>备份NameNode的元数据文件（dfs.name.dir）到SNN节点，或者其他多个节点。</p>

<h5 id="section-21">8.10 感知网络布局和机架的设计</h5>
<p><strong>DataNode数据块的副本策略</strong>
标准副本为3个：</p>

<p>第1个 在同一个DataNode上</p>

<p>第2个 不同的机架上的DataNode上</p>

<p>第3个 与第二个同机架的不同DataNode上</p>

<p>如果大于3个副本，其他的随机放置的不同节点上</p>

<p>topology.script.file.name</p>

<p>机架感知，配置Hadoop是NameNode知道DataNode的机架分布结构</p>

<h5 id="section-22">8.11 多用户作业调度</h5>
<p>8.11.1 多个JobTracker</p>

<p>8.11.2 公平调度器 （Facebook）</p>

<h4 id="hadoop-9">第九章 在云上运行Hadoop</h4>
<p><strong>租用 经济</strong>
Amazon Web Services(AWS)</p>

<p>Elastic Compute Clous(EC2) 弹性计算云</p>

<p>Simple Storage Service(S3)简单存储服务</p>

<p>1 AWS与外部网络通信的带宽需要收取费用，EC2内部不需要收费。</p>

<p>2 先存储在S3 在从S3中复制到主节点</p>

<p>S3中存储数据同样要收费 但是 存储空间可扩展、一次存储可以多次使用、资费相对低、S3和EC2是内部网，之间复制数据没有费用且速度快</p>

<p>3 还可以直接将S3作为输入输出文件地址，而不用复制数据到HDFS</p>

<h4 id="pignot-finished">第十章 用Pig编程（Not Finished）</h4>
<p>Pig是架构在Hadoop之上的高级数据处理层。</p>

<p><strong>易用性、高性能、大规模可扩展能力</strong>是所有Hadoop子项目的期望</p>

<p><strong>“Pig吃任何东西”</strong></p>

<p>Pig Latin命令运行： Grunt Shell、脚本文件、嵌入在Java程序中。</p>

<h4 id="hivehadoop">第十一章 Hive及Hadoop群</h4>
<p>Pig——一种高级数据流语言</p>

<p>Hive——一种类SQL数据仓库基础设施</p>

<p>HBase——一种模仿Google BigTable的分布式的、面向列的数据库</p>

<p>ZooKeeper——一种用于管理分布式应用之间共享状态的可靠的协同系统</p>

<p>Cascading——是Hadoop上用于组装和执行复杂数据处理工作流的一个API</p>

<p>Hama——矩阵计算软件包，用于计算乘积、逆、特征值、特征向量和其他矩阵运算</p>

<p>Mahout——基于Hadoop实现机器学习算法</p>

<h5 id="hive">Hive</h5>
<p>Hive是建立在Haddop基础之上的数据仓库软件包</p>

<p>HiveQL——类SQL的数据查询语言</p>

<p>MetaStore ——用于存放元数据的组件，存储在Derby关系数据库中（存取速度的考虑）</p>

<p><code>---EOF---</code></p>

]]></content>
  </entry>
  
</feed>
